{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install transformers pandas torch sentencepiece python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ABCD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(os.getenv('HF_TOKEN'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e7288abc0045e6afb7150dfbc40fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26accc1b8724653b40c5782642e591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d607fc884e54acaa8e92294d74d4cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25b6bd485f94a6c8e9178a08f3a6441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56337a9832d4800a223f850ee57ca19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fee7031b4304a72a934a87d79627d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'path_to_llama_model' with the actual path to your local LLaMA model\n",
    "model_path = \"meta-llama/Llama-3.2-1B\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(question, options):\n",
    "    prompt = f\"Question: {question}\\nOptions:\\n\"\n",
    "    for option, text in options.items():\n",
    "        prompt += f\"{option}. {text}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.7,\n",
    "        do_sample=False, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer using regex\n",
    "    match = re.search(r'Answer:\\s*([A-D])', output_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def clean_answer(text):\n",
    "    cleaned_text = re.sub(r'\\d', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Question 90/90\r"
     ]
    }
   ],
   "source": [
    "total_questions = len(df)\n",
    "correct_predictions = 0\n",
    "wrong_answers = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    question = row['Question:']\n",
    "    options = {\n",
    "        'A': row['Option A'],\n",
    "        'B': row['Option B'],\n",
    "        'C': row['Option C'],\n",
    "        'D': row['Option D']\n",
    "    }\n",
    "    correct_answer = row['Correct answer'].upper()\n",
    "\n",
    "    prediction = get_model_prediction(question, options)\n",
    "\n",
    "    if prediction == correct_answer:\n",
    "        correct_predictions += 1\n",
    "    else:\n",
    "        if len(wrong_answers) < 4:\n",
    "            wrong_answers.append({\n",
    "                'Question': question,\n",
    "                'Model Prediction': clean_answer(prediction),\n",
    "                'Correct Answer': correct_answer,\n",
    "                'Options': options\n",
    "            })\n",
    "\n",
    "    print(f\"Processed Question {index + 1}/{total_questions}\", end='\\r')\n",
    "\n",
    "accuracy = (correct_predictions / total_questions) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results:\n",
      "Total Questions: 90\n",
      "Correct Predictions: 36\n",
      "Accuracy: 40.00%\n",
      "\n",
      "Examples of Wrong Answers:\n",
      "\n",
      "Example 1:\n",
      "Question: Jaký máme rok?\n",
      "  A. 1876\n",
      "  B. 2014\n",
      "  C. 2023\n",
      "  D. 2024\n",
      "Model Prediction: C\n",
      "Correct Answer: D\n",
      "\n",
      "Example 2:\n",
      "Question: Kdo jako první vyřešil Basilejský problém?\n",
      "  A. Jacob Bernoulli\n",
      "  B. Leonard Euler\n",
      "  C. Karl Weierstrass\n",
      "  D.  Joseph-Louis Lagrange\n",
      "Model Prediction: D\n",
      "Correct Answer: B\n",
      "\n",
      "Example 3:\n",
      "Question: Jaký je nejstarší ze starověkých 7 divů světa? \n",
      "  A. Pyramidy v Gize\n",
      "  B. Diova socha v Olympii\n",
      "  C. Rhodsky kolos\n",
      "  D. Visuté zahrady Semiramidiny\n",
      "Model Prediction: B\n",
      "Correct Answer: A\n",
      "\n",
      "Example 4:\n",
      "Question: Která z následujících planet je nejblíže Slunci?\n",
      "  A. Země\n",
      "  B. Venuše\n",
      "  C. Merkur\n",
      "  D. Mars\n",
      "Model Prediction: D\n",
      "Correct Answer: C\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBenchmark Results:\")\n",
    "print(f\"Total Questions: {total_questions}\")\n",
    "print(f\"Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "if wrong_answers:\n",
    "    print(\"Examples of Wrong Answers:\")\n",
    "    for i, wa in enumerate(wrong_answers, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Question: {wa['Question']}\")\n",
    "        for opt, text in wa['Options'].items():\n",
    "            print(f\"  {opt}. {text}\")\n",
    "        print(f\"Model Prediction: {wa['Model Prediction']}\")\n",
    "        print(f\"Correct Answer: {wa['Correct Answer']}\")\n",
    "else:\n",
    "    print(\"No wrong answers found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
