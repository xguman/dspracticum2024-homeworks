{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ac2d70-6791-4b1f-b65d-b8c88ce0eeba",
   "metadata": {},
   "source": [
    "# HW06 - Training a small GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4ce989-ea7f-4c89-975c-046038996d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7cfa7-f664-4ea9-8f04-17ce270272de",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ec1afff-194b-461c-8073-75b769ccc0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredmluvaZasvätenie storočia Kuzmányho narodenia (r. 1906), ktoré obrátilo myseľ našu na tohoto\n",
      "         národného bohatiera a dalo podnet k hlbšiemu nahliadnutiu do jeho ducha a práce, priviedlo\n",
      "         nás ku vzácnemu poznaniu, že v Karlovi Kuzmánym máme veľkého ducha, ktorý zaujíma dôstojné\n",
      "         miesto v rade našich najprednejších repräzentantov, že je o moc väčší a vzácnejší, než sme\n",
      "         si ho dosiaľ kedykoľvek cenili. Tomuto poznaniu dal výraz Vajanský vo svojej prednáške: „i\n",
      "         vidíme, presvedčujeme sa o tom, čo sme dosiaľ len šípili, že máme v Kuzmánym veľkého\n",
      "         básnika, ktorého postaviť môžeme na rovný piedestál s Kollárom, Sládkovičom, Chalupkom.“[1]A nielen básnika veľkého máme v ňom. Práve tak veľkým sa dokázal ako kňaz, ako\n",
      "         národovec, ako organizátor, ako vedátor, ako vychovávateľ národa.Skutočne, keď sa vhĺbime do povahy, do duševnej dielne týchto otcov, patriarchov národa\n",
      "         nášho, ako bol Kuzmány a s ním Kollár, potom Lichard, Hodža, Hurban, Štúr a iní, prichodí\n",
      "         nám s hlbokou pokorou a obdivovaním vzdať úctu ich duchom. Ako boli ustarostení o svoj\n",
      "         národ! Len čiastku tej starosti a s ňou spojeného oduševnenia i konania, a národ náš musel\n",
      "         by prekvitať! Nemožno je sa diviť, že títo mužovia videli už brieždiť sa zore národného\n",
      "         slovenského života. Bol to optimizmus, založený na vnútornej sile, na sebadôvere. Vycítime\n",
      "         pri nich snahu, dopriať všetko dobré národu. Ba razom chceli potisnúť biedny národ\n",
      "         slovenský do popredia. Kde badali, že by bolo treba pomáhať, už tam boli. Vzory najnovšie\n",
      "         na každom poli, ešte sa len písalo o nich po časopisoch, už uvádzali i k nám. Dvíhať národ,\n",
      "         nahradiť v krátkosti tisícročnou nečinnosťou zameškané, to bola ich snaha, to struna, ktorá\n",
      "         ich ku každému činu roznietila.Z týchto mužov, skutočne otcov národa, bol Karol Kuzmány. Preto vďaka spolku\n",
      "         „Tranoscius“, že vypísaním súbehu, dal podnet a chuť k oceneniu práce jeho, k opísaniu jeho\n",
      "         života a jeho doby vzhľadom na náš národ a na jeho tohočasné dejiny.Zasluhuje si toho Kuzmány, ale žiada si a potrebuje to i sám národ. Lebo životopis\n",
      "         veľkých ľudí má veľký výchovný význam. Je v ňom istá príťažlivá sila. Mládež na príkladoch\n",
      "         otcov svojich sa vychováva. Preto bez nútenia rada číta životopisy veľkých mužov.S vedeckého však stanoviska životopis repräzentačného človeka má ten význam, že slúži na\n",
      "         objasnenie, na vysvetlenie diela, smýšľania, ducha jeho. Lebo k duchu diela prispieva\n",
      "         mnohým nálada, často len chvíľková a zavše dobová. Preto životopis má za povinnosť\n",
      "         vystihnúť náladu túto a tak prispieť k vysvetleniu diela, srozumiteľnejšou urobiť náladu,\n",
      "         pochopiteľnejším ducha, v diele sa javiaceho.Tieto dva významy životopisu, tieto dva požiadavky jeho budeme mať pred sebou pri ďalšej\n",
      "         rozprave. Takáto práca o Kuzmánym a jeho dobe je ťažká, je priekopná. Treba ísť k samým\n",
      "         prameňom, do starých knižníc. Málo sa dosiaľ napísalo o ňom, a i to je už potratené. Jeho\n",
      "         práce sú prachom zapadnuté, ak vôbec sú ešte zachované u niektorého bibliofila. I k tejto\n",
      "         práci len na viac miestach som môhol ako-tak posbierať diela Kuzmányho. Viacej pánov mi\n",
      "         láskave zapožičali knihy, ktoré zachovali v svojich knižniciach. I vnukovia veľkého Karla\n",
      "         mi s najväčšou ochotou prepustili všetko, čo sa u nich zachovalo, menovite vzácny, v\n",
      "         rukopise zachovaný životopis Karla Kuzmányho, pôvodne pre „Slavische Blätter“ napísaný, ale\n",
      "         podopĺňaný, potom familiárne dáta o rodine Kuzmányovskej.Všetkým ct. pánom, ktorí mi už čímkoľvek boli na pomoci, i na tomto mieste vzdávam\n",
      "         srdečnú vďaku.V Krupine, 1916.[1]Slov. Pohľ. 1906, str. 687.\n"
     ]
    }
   ],
   "source": [
    "# Replace with your own dataset\n",
    "dataset = load_dataset(\"xguman/hw5_text_dataset\")\n",
    "\n",
    "# Make validation split\n",
    "dataset['train'] = dataset['train'].select(range(20))\n",
    "dataset = dataset['train'].train_test_split(test_size=0.01)\n",
    "\n",
    "# Lower the training dataset to 500 rows\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "first_sentence = train_dataset['text'][0]\n",
    "print(first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "506ab2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available()  else device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db3e836a-8548-496b-bdaf-d2b98f5e2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82b1dfeb-3191-414e-b6b9-0b198946d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fa85a8235f414a8056da5919af7de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2201 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e06944592714a938b1f4e5b0054d70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_name', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file_name', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98cdb38c-eb4f-47e7-8c43-96bacb7826ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def concatenate_and_chunk(dataset, chunk_size=512):\n",
    "    # Flatten all `input_ids` into a single list\n",
    "    all_input_ids = list(chain(*dataset[\"input_ids\"]))\n",
    "    \n",
    "    # Create chunks of `chunk_size`\n",
    "    chunks = [all_input_ids[i:i + chunk_size] for i in range(0, len(all_input_ids), chunk_size)]\n",
    "    \n",
    "    # Only keep chunks that are exactly of length `chunk_size`\n",
    "    chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "    \n",
    "    # Create a new dataset with only the `input_ids` chunks\n",
    "    return Dataset.from_dict({\"input_ids\": chunks})\n",
    "\n",
    "# Apply this function to each split (train and test) in the DatasetDict\n",
    "chunked_ds = DatasetDict({\n",
    "    split: concatenate_and_chunk(split_ds, chunk_size=512)\n",
    "    for split, split_ds in tokenized_ds.items()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36f1ef42-f488-4ca0-af36-464f3e535be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator joins chunks into batches\n",
    "# see https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe26fe6-925f-49ec-a854-ebdb429c66ad",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37301b62-4bea-4006-aa1a-7785924227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration for the smallest GPT-2\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),      # Standard GPT-2 vocab size 50257\n",
    "    n_positions=512,                # Context size (512 is enough for small-scale models)\n",
    "    n_embd=768,                     # Embedding size\n",
    "    n_layer=12,                     # Number of transformer layers\n",
    "    n_head=12,                      # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = GPT2LMHeadModel(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7a48b42-9ce2-433f-acff-ce694449c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Define the perplexity metric\n",
    "def compute_metrics(eval_pred):\n",
    "    # `eval_pred` is a tuple of (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # Shift labels so that tokens align for calculating loss\n",
    "    shift_labels = labels[:, 1:].reshape(-1)\n",
    "    shift_logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\"perplexity\": perplexity}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4c203-9d5d-494e-b425-4b5a265516ff",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1ff634b-e879-44ad-a6d1-21c0a573aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2003/3246992496.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    }
   ],
   "source": [
    "# Set this according to size of your dataset\n",
    "# You should train for at least 15 mins on A10 GPU to get something reasonable\n",
    "TRAIN_EPOCHS = 200\n",
    "\n",
    "SAVE_STEPS = 1000\n",
    "EVAL_STEPS = SAVE_STEPS // 2\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-training\",  # Directory to save the model checkpoints and other outputs\n",
    "    eval_strategy=\"steps\",  # Evaluation strategy to use during training ('steps' or 'epochs')\n",
    "    eval_steps=EVAL_STEPS,  # Perform evaluation every EVAL_STEPS steps\n",
    "    num_train_epochs=TRAIN_EPOCHS,  # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation on each device\n",
    "    learning_rate=2.5e-4,  # Initial learning rate for the optimizer\n",
    "    lr_scheduler_type='cosine',  # Learning rate scheduler type. 'cosine' provides a cosine decay schedule.\n",
    "    warmup_ratio=0.05,  # Proportion of training to perform linear learning rate warmup for\n",
    "    adam_beta1=0.9,  # Beta1 parameter for the Adam optimizer (first moment decay)\n",
    "    adam_beta2=0.999,  # Beta2 parameter for the Adam optimizer (second moment decay)\n",
    "    weight_decay=0.01,  # Weight decay to apply (L2 regularization)\n",
    "    logging_strategy=\"steps\",  # Logging strategy to use. 'steps' logs at specified steps.\n",
    "    logging_steps=EVAL_STEPS,  # Log training metrics every EVAL_STEPS steps\n",
    "    save_steps=SAVE_STEPS,  # Save a checkpoint every SAVE_STEPS steps\n",
    "    save_total_limit=10,  # Maximum number of checkpoints to keep. Older checkpoints are deleted.\n",
    "    # report_to='wandb',  # Uncomment to report metrics to Weights and Biases (optional)\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds[\"train\"],\n",
    "                 eval_dataset=chunked_ds[\"test\"],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 data_collator = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87c7eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 20:33, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.107100</td>\n",
       "      <td>5.697019</td>\n",
       "      <td>297.972577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>6.599790</td>\n",
       "      <td>734.927239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>6.939453</td>\n",
       "      <td>1032.187362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>7.223747</td>\n",
       "      <td>1371.593740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>7.536597</td>\n",
       "      <td>1875.410686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>7.608286</td>\n",
       "      <td>2014.799442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>7.654192</td>\n",
       "      <td>2109.445776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3600, training_loss=0.4683267268869612, metrics={'train_runtime': 1234.3527, 'train_samples_per_second': 45.368, 'train_steps_per_second': 2.917, 'total_flos': 1.4632353792e+16, 'train_loss': 0.4683267268869612, 'epoch': 200.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54b62744-896e-4faf-b21b-3e031dc1217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gpt2-small-final\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01639456-2124-46e8-8d6c-46b4cbd5c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c93208042a41eb8f0123400b28035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/xguman/my_small_gpt2_zlatyfond/commit/662bfc530658b1cf51663fbb905f664d4b8727a6', commit_message='Upload tokenizer', commit_description='', oid='662bfc530658b1cf51663fbb905f664d4b8727a6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/xguman/my_small_gpt2_zlatyfond', endpoint='https://huggingface.co', repo_type='model', repo_id='xguman/my_small_gpt2_zlatyfond'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_MODEL_NAME = \"my_small_gpt2_zlatyfond\" # change this\n",
    "HF_TOKEN = \"\" #todo change this\n",
    "\n",
    "model.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6dac4-f46e-42a2-833f-07cb16eab3bc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now you can switch from GPU to CPU. Try to complete some prompt specific to your dataset.\n",
    "\n",
    "Does it make sense? Is it at least in Czech/Slovak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f9ade4a-f8b3-441d-8b6f-550048a21f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9ee0298-2d7f-4aa4-9a2d-13429b7e197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model =  GPT2LMHeadModel.from_pretrained(\"./gpt2-small-final\").to(\"cpu\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e42697e6-10ad-4ea8-a9a9-addef655a1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Hradné\\n         okamženie, od roku 1868, keď zazneli prvé, neisté zvuky jeho lýry,['}],\n",
       " [{'generated_text': 'Treba pomáhať sa zašady n prascuračene, svojho pobr zať sa k                '}],\n",
       " [{'generated_text': 'prepustili, zám na nad svojúmí, čočnej pieseobodaj                      '}]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = [\"Hrad\", \"Treba pomáhať\", \"prepustili\"] # Set starting prompt, something specific for your dataset\n",
    "\n",
    "generator(\n",
    "    PROMPT,\n",
    "    max_length=50,       # Maximum length of the generated text\n",
    "    do_sample=True,\n",
    "    temperature=0.7,        \n",
    "    repetition_penalty=1,  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
